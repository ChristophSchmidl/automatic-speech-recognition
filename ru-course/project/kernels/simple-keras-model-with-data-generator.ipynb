{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Input, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    ''' Returns dataframe with columns: 'path', 'word'.'''\n",
    "    datadir = Path(path)\n",
    "    files = [(str(f), f.parts[-2]) for f in datadir.glob('**/*.wav') if f]\n",
    "    df = pd.DataFrame(files, columns=['path', 'word'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    '''Transform data into something more useful.'''\n",
    "    train_words = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "    words = df.word.unique().tolist()\n",
    "    silence = ['_background_noise_']\n",
    "    unknown = [w for w in words if w not in silence + train_words]\n",
    "\n",
    "    # there are only 6 silence files. Mark them as unknown too.\n",
    "    df.loc[df.word.isin(silence), 'word'] = 'unknown'\n",
    "    df.loc[df.word.isin(unknown), 'word'] = 'unknown'\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_specgrams(paths, nsamples=16000):\n",
    "    '''\n",
    "    Given list of paths, return specgrams.\n",
    "    '''\n",
    "    \n",
    "    # read the wav files\n",
    "    wavs = [wavfile.read(x)[1] for x in paths]\n",
    "\n",
    "    # zero pad the shorter samples and cut off the long ones.\n",
    "    data = [] \n",
    "    for wav in wavs:\n",
    "        if wav.size < 16000:\n",
    "            d = np.pad(wav, (nsamples - wav.size, 0), mode='constant')\n",
    "        else:\n",
    "            d = wav[0:nsamples]\n",
    "        data.append(d)\n",
    "\n",
    "    # get the specgram\n",
    "    specgram = [signal.spectrogram(d, nperseg=256, noverlap=128)[2] for d in data]\n",
    "    specgram = [s.reshape(129, 124, -1) for s in specgram]\n",
    "    \n",
    "    return specgram\n",
    "\n",
    "\n",
    "def batch_generator(X, y, batch_size=16):\n",
    "    '''\n",
    "    Return a random image from X, y\n",
    "    '''\n",
    "    \n",
    "    while True:\n",
    "        # choose batch_size random images / labels from the data\n",
    "        idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "        im = X[idx]\n",
    "        label = y[idx]\n",
    "        \n",
    "        specgram = get_specgrams(im)\n",
    "\n",
    "\n",
    "        yield np.concatenate([specgram]), label\n",
    "\n",
    "\n",
    "def get_model(shape):\n",
    "    '''Create a keras model.'''\n",
    "    inputlayer = Input(shape=shape)\n",
    "\n",
    "    model = BatchNormalization()(inputlayer)\n",
    "    model = Conv2D(16, (3, 3), activation='elu')(model)\n",
    "    model = Dropout(0.25)(model)\n",
    "    model = MaxPooling2D((2, 2))(model)\n",
    "\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(32, activation='elu')(model)\n",
    "    model = Dropout(0.25)(model)\n",
    "    \n",
    "    # 11 because background noise has been taken out\n",
    "    model = Dense(11, activation='sigmoid')(model)\n",
    "    \n",
    "    model = Model(inputs=inputlayer, outputs=model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64727, 2)\n",
      "Epoch 1/30\n",
      "1414/1415 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/lib/python3.7/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415/1415 [==============================] - 77s 54ms/step - loss: 0.2027 - acc: 0.9328 - val_loss: 0.1999 - val_acc: 0.9359\n",
      "Epoch 2/30\n",
      "1415/1415 [==============================] - 71s 50ms/step - loss: 0.1789 - acc: 0.9373 - val_loss: 0.1997 - val_acc: 0.9358\n",
      "Epoch 3/30\n",
      "1415/1415 [==============================] - 82s 58ms/step - loss: 0.1696 - acc: 0.9394 - val_loss: 0.1843 - val_acc: 0.9369\n",
      "Epoch 4/30\n",
      "1415/1415 [==============================] - 75s 53ms/step - loss: 0.1629 - acc: 0.9408 - val_loss: 0.1997 - val_acc: 0.9367\n",
      "Epoch 5/30\n",
      "1415/1415 [==============================] - 88s 62ms/step - loss: 0.1588 - acc: 0.9428 - val_loss: 0.1868 - val_acc: 0.9401\n",
      "Epoch 6/30\n",
      "1415/1415 [==============================] - 87s 61ms/step - loss: 0.1591 - acc: 0.9429 - val_loss: 0.1881 - val_acc: 0.9351\n",
      "Epoch 7/30\n",
      "1415/1415 [==============================] - 88s 62ms/step - loss: 0.1508 - acc: 0.9447 - val_loss: 0.1955 - val_acc: 0.9390\n",
      "Epoch 8/30\n",
      "1415/1415 [==============================] - 84s 59ms/step - loss: 0.1485 - acc: 0.9458 - val_loss: 0.1968 - val_acc: 0.9393\n",
      "Epoch 9/30\n",
      "1415/1415 [==============================] - 84s 60ms/step - loss: 0.1433 - acc: 0.9475 - val_loss: 0.1848 - val_acc: 0.9407\n",
      "Epoch 10/30\n",
      "1415/1415 [==============================] - 89s 63ms/step - loss: 0.1405 - acc: 0.9486 - val_loss: 0.1860 - val_acc: 0.9412\n",
      "Epoch 11/30\n",
      "1415/1415 [==============================] - 86s 61ms/step - loss: 0.1401 - acc: 0.9485 - val_loss: 0.1807 - val_acc: 0.9416\n",
      "Epoch 12/30\n",
      "1415/1415 [==============================] - 80s 57ms/step - loss: 0.1371 - acc: 0.9494 - val_loss: 0.1960 - val_acc: 0.9399\n",
      "Epoch 13/30\n",
      "1415/1415 [==============================] - 84s 59ms/step - loss: 0.1663 - acc: 0.9443 - val_loss: 0.1828 - val_acc: 0.9390\n",
      "Epoch 14/30\n",
      "1415/1415 [==============================] - 81s 58ms/step - loss: 0.1378 - acc: 0.9503 - val_loss: 0.1903 - val_acc: 0.9393\n",
      "Epoch 15/30\n",
      "1415/1415 [==============================] - 78s 55ms/step - loss: 0.1309 - acc: 0.9516 - val_loss: 0.1831 - val_acc: 0.9424\n",
      "Epoch 16/30\n",
      "1415/1415 [==============================] - 78s 55ms/step - loss: 0.1304 - acc: 0.9521 - val_loss: 0.1908 - val_acc: 0.9415\n",
      "Epoch 17/30\n",
      "1415/1415 [==============================] - 71s 50ms/step - loss: 0.1281 - acc: 0.9532 - val_loss: 0.1849 - val_acc: 0.9429\n",
      "Epoch 18/30\n",
      "1415/1415 [==============================] - 65s 46ms/step - loss: 0.1274 - acc: 0.9528 - val_loss: 0.2025 - val_acc: 0.9388\n",
      "Epoch 19/30\n",
      "1415/1415 [==============================] - 69s 49ms/step - loss: 0.1261 - acc: 0.9539 - val_loss: 0.1973 - val_acc: 0.9412\n",
      "Epoch 20/30\n",
      "1415/1415 [==============================] - 67s 48ms/step - loss: 0.1252 - acc: 0.9544 - val_loss: 0.1996 - val_acc: 0.9390\n",
      "Epoch 21/30\n",
      "1415/1415 [==============================] - 71s 50ms/step - loss: 0.1256 - acc: 0.9536 - val_loss: 0.1909 - val_acc: 0.9417\n",
      "Epoch 22/30\n",
      "1415/1415 [==============================] - 68s 48ms/step - loss: 0.1237 - acc: 0.9546 - val_loss: 0.1944 - val_acc: 0.9410\n",
      "Epoch 23/30\n",
      "1415/1415 [==============================] - 77s 55ms/step - loss: 0.1229 - acc: 0.9551 - val_loss: 0.2033 - val_acc: 0.9382\n",
      "Epoch 24/30\n",
      "1415/1415 [==============================] - 75s 53ms/step - loss: 0.1225 - acc: 0.9552 - val_loss: 0.2101 - val_acc: 0.9392\n",
      "Epoch 25/30\n",
      "1415/1415 [==============================] - 75s 53ms/step - loss: 0.1214 - acc: 0.9555 - val_loss: 0.1981 - val_acc: 0.9436\n",
      "Epoch 26/30\n",
      "1415/1415 [==============================] - 75s 53ms/step - loss: 0.1202 - acc: 0.9557 - val_loss: 0.2046 - val_acc: 0.9403\n",
      "Epoch 27/30\n",
      "1415/1415 [==============================] - 85s 60ms/step - loss: 0.1184 - acc: 0.9564 - val_loss: 0.1955 - val_acc: 0.9423\n",
      "Epoch 28/30\n",
      "1415/1415 [==============================] - 83s 59ms/step - loss: 0.1182 - acc: 0.9569 - val_loss: 0.2115 - val_acc: 0.9387\n",
      "Epoch 29/30\n",
      "1415/1415 [==============================] - 83s 59ms/step - loss: 0.1173 - acc: 0.9574 - val_loss: 0.2086 - val_acc: 0.9393\n",
      "Epoch 30/30\n",
      "1415/1415 [==============================] - 70s 49ms/step - loss: 0.1169 - acc: 0.9572 - val_loss: 0.2172 - val_acc: 0.9361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01c06a7e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = prepare_data(get_data('../data/train/audio/'))\n",
    "shape = (129, 124, 1)\n",
    "model = get_model(shape)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "# create training and test data.\n",
    "\n",
    "labelbinarizer = LabelBinarizer()\n",
    "X = train.path\n",
    "y = labelbinarizer.fit_transform(train.word)\n",
    "X, Xt, y, yt = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/{}'.format(time.time()), batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "train_gen = batch_generator(X.values, y, batch_size=32)\n",
    "valid_gen = batch_generator(Xt.values, yt, batch_size=32)\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    epochs=30,\n",
    "    steps_per_epoch=X.shape[0] // 32,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=Xt.shape[0] // 32,\n",
    "    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission\n",
    "# Scores on the leaderboard:\n",
    "# 0.10389 with 1 epoch \n",
    "# 0.33717 with 30 epochs\n",
    "\n",
    "\n",
    "\n",
    "test = prepare_data(get_data('../data/test/audio/'))\n",
    "\n",
    "predictions = []\n",
    "paths = test.path.tolist()\n",
    "\n",
    "for path in paths:\n",
    "    specgram = get_specgrams([path])\n",
    "    pred = model.predict(np.array(specgram))\n",
    "    predictions.extend(pred)\n",
    "\n",
    "\n",
    "labels = [labelbinarizer.inverse_transform(p.reshape(1, -1), threshold=0.5)[0] for p in predictions]\n",
    "test['labels'] = labels\n",
    "test.path = test.path.apply(lambda x: str(x).split('/')[-1])\n",
    "submission = pd.DataFrame({'fname': test.path.tolist(), 'label': labels})\n",
    "submission.to_csv('simple-keras-model-with-data-generator_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
