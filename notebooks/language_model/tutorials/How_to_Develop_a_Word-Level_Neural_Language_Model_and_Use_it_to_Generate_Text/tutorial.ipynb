{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Develop a Word-Level Neural Language Model and Use it to Generate Text\n",
    "\n",
    "* Tutorial website: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "* In this tutorial, we will develop a model of the text that we can then use to generate new sequences of text.\n",
    "* Based on the corpus/text \"The Republic\" by Plato: http://www.gutenberg.org/cache/epub/1497/pg1497.txt\n",
    "* The cleaned version is called republic_clean.txt and can be found under \"data\" in this repo\n",
    "* Salient characteristics of the text:\n",
    "    * Book/Chapter headings (e.g. “BOOK I.”).\n",
    "    * British English spelling (e.g. “honoured”)\n",
    "    * Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)\n",
    "    * Strange names (e.g. “Polemarchus”).\n",
    "    * Some long monologues that go on for hundreds of lines.\n",
    "    * Some quoted dialog (e.g. ‘…’)\n",
    "* We will pick a length of 50 words for the length of the input sequences, somewhat arbitrarily.  \n",
    "* Now that we have a model design, we can look at transforming the raw text into sequences of 50 input words to 1 output word, ready to fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/cs/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wha\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "# print the first 200 characters\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
      "Total Tokens: 118683\n",
      "Unique Tokens: 7409\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 118632\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'data/republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)\n",
    "\n",
    "# You will see that each line is shifted along one word, with a new word at the end to be predicted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'data/republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "# https://keras.io/preprocessing/text/#tokenizer\n",
    "# We can access the mapping of words to integers as a \n",
    "# dictionary attribute called word_index on the Tokenizer object.\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118632/118632 [==============================] - 139s 1ms/step - loss: 6.1912 - acc: 0.0654\n",
      "Epoch 2/100\n",
      "118632/118632 [==============================] - 141s 1ms/step - loss: 5.7830 - acc: 0.0993\n",
      "Epoch 3/100\n",
      "118632/118632 [==============================] - 141s 1ms/step - loss: 5.5678 - acc: 0.1175\n",
      "Epoch 4/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 5.4162 - acc: 0.1351\n",
      "Epoch 5/100\n",
      "118632/118632 [==============================] - 138s 1ms/step - loss: 5.2990 - acc: 0.1459\n",
      "Epoch 6/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 5.2394 - acc: 0.1489\n",
      "Epoch 7/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 5.1698 - acc: 0.1548\n",
      "Epoch 8/100\n",
      "118632/118632 [==============================] - 144s 1ms/step - loss: 5.0919 - acc: 0.1600\n",
      "Epoch 9/100\n",
      "118632/118632 [==============================] - 138s 1ms/step - loss: 5.0259 - acc: 0.1651\n",
      "Epoch 10/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.9445 - acc: 0.1693\n",
      "Epoch 11/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.8727 - acc: 0.1731\n",
      "Epoch 12/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.7986 - acc: 0.1777\n",
      "Epoch 13/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.7208 - acc: 0.1834\n",
      "Epoch 14/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.6463 - acc: 0.1868\n",
      "Epoch 15/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.5746 - acc: 0.1916\n",
      "Epoch 16/100\n",
      "118632/118632 [==============================] - 139s 1ms/step - loss: 4.5590 - acc: 0.1908\n",
      "Epoch 17/100\n",
      "118632/118632 [==============================] - 146s 1ms/step - loss: 4.4652 - acc: 0.1961\n",
      "Epoch 18/100\n",
      "118632/118632 [==============================] - 140s 1ms/step - loss: 4.3943 - acc: 0.1996\n",
      "Epoch 19/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.3298 - acc: 0.2027\n",
      "Epoch 20/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.2705 - acc: 0.2058\n",
      "Epoch 21/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.2189 - acc: 0.2086\n",
      "Epoch 22/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.1585 - acc: 0.2119\n",
      "Epoch 23/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.1103 - acc: 0.2146\n",
      "Epoch 24/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.1262 - acc: 0.2139\n",
      "Epoch 25/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.1665 - acc: 0.2115\n",
      "Epoch 26/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.2197 - acc: 0.2082\n",
      "Epoch 27/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.1582 - acc: 0.2120\n",
      "Epoch 28/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 4.0671 - acc: 0.2185\n",
      "Epoch 29/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.9779 - acc: 0.2243\n",
      "Epoch 30/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.9285 - acc: 0.2290\n",
      "Epoch 31/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8866 - acc: 0.2329\n",
      "Epoch 32/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8797 - acc: 0.2333\n",
      "Epoch 33/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8189 - acc: 0.2394\n",
      "Epoch 34/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.7865 - acc: 0.2433\n",
      "Epoch 35/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8419 - acc: 0.2390\n",
      "Epoch 36/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8311 - acc: 0.2398\n",
      "Epoch 37/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8695 - acc: 0.2366\n",
      "Epoch 38/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.8390 - acc: 0.2387\n",
      "Epoch 39/100\n",
      "118632/118632 [==============================] - 133s 1ms/step - loss: 3.7869 - acc: 0.2450\n",
      "Epoch 40/100\n",
      "118632/118632 [==============================] - 135s 1ms/step - loss: 3.7610 - acc: 0.2468\n",
      "Epoch 41/100\n",
      "118632/118632 [==============================] - 140s 1ms/step - loss: 3.7218 - acc: 0.2510\n",
      "Epoch 42/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 3.6839 - acc: 0.2552\n",
      "Epoch 43/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.6845 - acc: 0.2550\n",
      "Epoch 44/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.6868 - acc: 0.2550\n",
      "Epoch 45/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.6518 - acc: 0.2589\n",
      "Epoch 46/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.6221 - acc: 0.2627\n",
      "Epoch 47/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.5951 - acc: 0.2657\n",
      "Epoch 48/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 3.5819 - acc: 0.2678\n",
      "Epoch 49/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.5245 - acc: 0.2748\n",
      "Epoch 50/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 3.4985 - acc: 0.2772\n",
      "Epoch 51/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.4662 - acc: 0.2834\n",
      "Epoch 52/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.4282 - acc: 0.2884\n",
      "Epoch 53/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.4091 - acc: 0.2902\n",
      "Epoch 54/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.3760 - acc: 0.2943\n",
      "Epoch 55/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.3398 - acc: 0.2990\n",
      "Epoch 56/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.3404 - acc: 0.2995\n",
      "Epoch 57/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.2881 - acc: 0.3067\n",
      "Epoch 58/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.2562 - acc: 0.3114\n",
      "Epoch 59/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.2314 - acc: 0.3151\n",
      "Epoch 60/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 3.2488 - acc: 0.3132\n",
      "Epoch 61/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.2140 - acc: 0.3181\n",
      "Epoch 62/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.2246 - acc: 0.3174\n",
      "Epoch 63/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.1827 - acc: 0.3231\n",
      "Epoch 64/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.1677 - acc: 0.3260\n",
      "Epoch 65/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.1666 - acc: 0.3272\n",
      "Epoch 66/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.1535 - acc: 0.3288\n",
      "Epoch 67/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.1259 - acc: 0.3320\n",
      "Epoch 68/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.0804 - acc: 0.3377\n",
      "Epoch 69/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.0379 - acc: 0.3440\n",
      "Epoch 70/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.0026 - acc: 0.3497\n",
      "Epoch 71/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.9845 - acc: 0.3524\n",
      "Epoch 72/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.9596 - acc: 0.3557\n",
      "Epoch 73/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.9398 - acc: 0.3606\n",
      "Epoch 74/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.9311 - acc: 0.3609\n",
      "Epoch 75/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.9236 - acc: 0.3640\n",
      "Epoch 76/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.0868 - acc: 0.3453\n",
      "Epoch 77/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 3.0509 - acc: 0.3479\n",
      "Epoch 78/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.9261 - acc: 0.3651\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118632/118632 [==============================] - 136s 1ms/step - loss: 3.0575 - acc: 0.3419\n",
      "Epoch 80/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.8562 - acc: 0.3733\n",
      "Epoch 81/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.8601 - acc: 0.3733\n",
      "Epoch 82/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.8372 - acc: 0.3788\n",
      "Epoch 83/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.8545 - acc: 0.3785\n",
      "Epoch 84/100\n",
      "118632/118632 [==============================] - 146s 1ms/step - loss: 2.9171 - acc: 0.3628\n",
      "Epoch 85/100\n",
      "118632/118632 [==============================] - 149s 1ms/step - loss: 2.8946 - acc: 0.3693\n",
      "Epoch 86/100\n",
      "118632/118632 [==============================] - 140s 1ms/step - loss: 2.7893 - acc: 0.3854\n",
      "Epoch 87/100\n",
      "118632/118632 [==============================] - 139s 1ms/step - loss: 2.7580 - acc: 0.3922\n",
      "Epoch 88/100\n",
      "118632/118632 [==============================] - 142s 1ms/step - loss: 2.7390 - acc: 0.3939\n",
      "Epoch 89/100\n",
      "118632/118632 [==============================] - 163s 1ms/step - loss: 2.7035 - acc: 0.4020\n",
      "Epoch 90/100\n",
      "118632/118632 [==============================] - 172s 1ms/step - loss: 2.6754 - acc: 0.4073\n",
      "Epoch 91/100\n",
      "118632/118632 [==============================] - 139s 1ms/step - loss: 2.8255 - acc: 0.3877\n",
      "Epoch 92/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.7516 - acc: 0.3967\n",
      "Epoch 93/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.7371 - acc: 0.3999\n",
      "Epoch 94/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.6911 - acc: 0.4053\n",
      "Epoch 95/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.6410 - acc: 0.4122\n",
      "Epoch 96/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.6171 - acc: 0.4164\n",
      "Epoch 97/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.6576 - acc: 0.4139\n",
      "Epoch 98/100\n",
      "118632/118632 [==============================] - 136s 1ms/step - loss: 2.5991 - acc: 0.4212\n",
      "Epoch 99/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.5865 - acc: 0.4248\n",
      "Epoch 100/100\n",
      "118632/118632 [==============================] - 137s 1ms/step - loss: 2.5717 - acc: 0.4248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe790537358>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "# takes several hours on a GTX 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('data/model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('data/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'data/republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('data/model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('data/tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no one will argue that there is any other method of comprehending by any regular process all true existence or of ascertaining what each thing is in its own nature for the arts in general are concerned with the desires or opinions of men or are cultivated with a view to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "[47, 33, 18, 977, 9, 36, 5, 45, 42, 1193, 3, 2978, 27, 45, 2605, 745, 37, 38, 383, 17, 3, 6335, 30, 188, 151, 5, 6, 699, 82, 94, 26, 1, 286, 6, 339, 14, 567, 35, 1, 233, 17, 1141, 3, 77, 17, 14, 2227, 35, 8, 230, 4]\n"
     ]
    }
   ],
   "source": [
    "# [0] = get the first element of the list / flatten to one-dimensional list\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "print(len(encoded))\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array(encoded)\n",
    "encoded = encoded[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected embedding_1_input to have shape (50,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-445860419a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predict probabilities for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \"\"\"\n\u001b[1;32m   1137\u001b[0m         proba = self.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1138\u001b[0;31m                              steps=steps)\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected embedding_1_input to have shape (50,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "# Throws an error: ValueError: Error when checking : expected embedding_1_input to have shape (50,) but got array with shape (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
